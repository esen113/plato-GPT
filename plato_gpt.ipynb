{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Building plato GPT from scrach using tensor flow**"
      ],
      "metadata": {
        "id": "YCbiT3DK8qDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vLwFUQ0f8nLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/plato.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"length of dataset in characters:\", len(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMtHMBCE65ul",
        "outputId": "f3783c85-52d2-48aa-a698-ad37fc3ef22a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "length of dataset in characters: 263400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = text.splitlines()\n",
        "\n",
        "# print 5\n",
        "for line in lines[:5]:\n",
        "    print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJVyLHB_HGtH",
        "outputId": "d6cbd029-b567-4577-e1ce-5192e7e49c61"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿I went down yesterday to the Piraeus with Glaucon the son of Ariston, that I might offer up my prayers to the goddess (Bendis, the Thracian Artemis.); and also because I wanted to see in what manner they would celebrate the festival, which was a new thing. I was delighted with the procession of the inhabitants; but that of the Thracians was equally, if not more, beautiful. When we had finished our prayers and viewed the spectacle, we turned in the direction of the city; and at that instant Polemarchus the son of Cephalus chanced to catch sight of us from a distance as we were starting on our way home, and told his servant to run and bid us wait for him. The servant took hold of me by the cloak behind, and said: Polemarchus desires you to wait.\n",
            "I turned round, and asked him where his master was.\n",
            "There he is, said the youth, coming after you, if you will only wait.\n",
            "Certainly we will, said Glaucon; and in a few minutes Polemarchus appeared, and with him Adeimantus, Glaucon’s brother, Niceratus the son of Nicias, and several others who had been at the procession.\n",
            "Polemarchus said to me: I perceive, Socrates, that you and your companion are already on your way to the city.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# world embedding**"
      ],
      "metadata": {
        "id": "_4FT6MVjPouE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTU2gqv0MYLj",
        "outputId": "c20f4003-2895-4114-cbf9-28bd64f4bb28"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "model_prefix = \"plato_bpe\"\n",
        "\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=file_path,\n",
        "    model_prefix=model_prefix,\n",
        "    vocab_size=8000,        #\n",
        "    model_type='bpe',       #\n",
        "    character_coverage=1.0, #\n",
        "    input_sentence_size=100000,\n",
        "    shuffle_input_sentence=True\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MMh95qd4Mbhq"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(f\"{model_prefix}.model\")\n",
        "\n",
        "test_str = \"Socrates: Let us discuss justice.\"\n",
        "tokens = sp.encode(test_str, out_type=str)\n",
        "print(\"tokens：\", tokens)\n",
        "\n",
        "print(\"BPE size：\", sp.vocab_size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VM43ZRINaWB",
        "outputId": "1e716dd5-6c01-48fe-82b3-0e7add84b68f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens： ['▁Socrates', ':', '▁Let', '▁us', '▁discus', 's', '▁justice', '.']\n",
            "BPE size： 8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# transfer\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "# tokenize\n",
        "encoded_ids = sp.encode(text_data, out_type=int)\n",
        "print(\"Token总数:\", len(encoded_ids))\n",
        "\n",
        "#numpy\n",
        "encoded_ids = np.array(encoded_ids, dtype=np.int32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQcqVVbTNuus",
        "outputId": "4d50a14d-97f4-49b1-f458-6b7113177274"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token总数: 57064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "set up training data"
      ],
      "metadata": {
        "id": "ur_RHW96PwJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how many tokens in a sensence\n",
        "seq_len = 64\n",
        "\n",
        "# make sure //\n",
        "total_tokens = len(encoded_ids)\n",
        "num_subsequences = total_tokens // (seq_len + 1)\n",
        "trimmed_size = num_subsequences * (seq_len + 1)\n",
        "\n",
        "# get rid of //\n",
        "encoded_ids = encoded_ids[:trimmed_size]\n",
        "\n",
        "# 3.2 reshape\n",
        "# 我们把它 reshape 成 [num_subsequences, seq_len+1]\n",
        "subsequences = encoded_ids.reshape((num_subsequences, seq_len + 1))\n",
        "\n",
        "# 3.3  tf.data.Dataset\n",
        "def split_input_target(seq):\n",
        "\n",
        "    input_seq = seq[:-1]\n",
        "    target_seq = seq[1:]\n",
        "    return input_seq, target_seq\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(subsequences)\n",
        "dataset = dataset.map(split_input_target)\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print(\"Dataset structure：\", dataset.element_spec)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdZUzBgGN5Gx",
        "outputId": "236915e9-a66a-4fc3-c040-6be72dde2bd5"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset structure： (TensorSpec(shape=(32, 64), dtype=tf.int32, name=None), TensorSpec(shape=(32, 64), dtype=tf.int32, name=None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **# multi_head**"
      ],
      "metadata": {
        "id": "Y0OaS2_pR-Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "\n",
        "        self.query_dense = tf.keras.layers.Dense(embed_dim)\n",
        "        self.key_dense   = tf.keras.layers.Dense(embed_dim)\n",
        "        self.value_dense = tf.keras.layers.Dense(embed_dim)\n",
        "        self.out_dense   = tf.keras.layers.Dense(embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        seq_len    = tf.shape(x)[1]\n",
        "\n",
        "        # Q, K, V\n",
        "        q = self.query_dense(x)\n",
        "        k = self.key_dense(x)\n",
        "        v = self.value_dense(x)\n",
        "\n",
        "        # reshape: (batch, seq_len, num_heads, projection_dim)\n",
        "        q = tf.reshape(q, (batch_size, seq_len, self.num_heads, self.projection_dim))\n",
        "        k = tf.reshape(k, (batch_size, seq_len, self.num_heads, self.projection_dim))\n",
        "        v = tf.reshape(v, (batch_size, seq_len, self.num_heads, self.projection_dim))\n",
        "\n",
        "        # : (batch, num_heads, seq_len, projection_dim)\n",
        "        q = tf.transpose(q, [0, 2, 1, 3])\n",
        "        k = tf.transpose(k, [0, 2, 1, 3])\n",
        "        v = tf.transpose(v, [0, 2, 1, 3])\n",
        "\n",
        "        # get weight\n",
        "        scale = tf.cast(self.projection_dim, tf.float32) ** 0.5\n",
        "        logits = tf.matmul(q, k, transpose_b=True) / scale  # (batch, heads, seq_len, seq_len)\n",
        "\n",
        "        # mask\n",
        "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)  # 下三角=1，上三角=0\n",
        "        mask = tf.reshape(mask, (1, 1, seq_len, seq_len))               # (1,1,seq_len,seq_len)\n",
        "\n",
        "        # prepare for softmax\n",
        "        logits = logits * mask + (1.0 - mask) * -1e9\n",
        "\n",
        "        weights = tf.nn.softmax(logits, axis=-1)     # (batch, heads, seq_len, seq_len)\n",
        "        attention_output = tf.matmul(weights, v)     # (batch, heads, seq_len, projection_dim)\n",
        "\n",
        "        #\n",
        "        attention_output = tf.transpose(attention_output, [0, 2, 1, 3])\n",
        "        #  (batch, seq_len, num_heads, projection_dim)\n",
        "\n",
        "        concat_output = tf.reshape(attention_output, (batch_size, seq_len, self.embed_dim))\n",
        "\n",
        "        #  Dense\n",
        "        out = self.out_dense(concat_output)  # (batch, seq_len, embed_dim)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "lIhrJNBdP3uA"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tramsfomer"
      ],
      "metadata": {
        "id": "1w6N8BVgTbfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = CausalSelfAttention(embed_dim, num_heads)\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.ff = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
        "            tf.keras.layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        #self attention\n",
        "        attn_output = self.att(x)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.norm1(x + attn_output)\n",
        "\n",
        "        # feed foward\n",
        "        ff_output = self.ff(out1)\n",
        "        ff_output = self.dropout2(ff_output, training=training)\n",
        "        out2 = self.norm2(out1 + ff_output)\n",
        "\n",
        "        return out2\n"
      ],
      "metadata": {
        "id": "pHgQdR7hSJSX"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleGPT(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 max_seq_len=64,\n",
        "                 embed_dim=128,\n",
        "                 num_heads=4,\n",
        "                 ff_dim=256,\n",
        "                 num_layers=2,\n",
        "                 dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        # word embedding\n",
        "        self.token_embed = tf.keras.layers.Embedding(vocab_size, embed_dim)\n",
        "        # position embedding\n",
        "        self.pos_embed   = tf.keras.layers.Embedding(input_dim=max_seq_len, output_dim=embed_dim)\n",
        "\n",
        "        #  TransformerBlock from class TransformerBlock(tf.keras.layers.Layer)\n",
        "        self.blocks = [\n",
        "            TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.fc_out  = tf.keras.layers.Dense(vocab_size)\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        seq_len    = tf.shape(x)[1]\n",
        "\n",
        "        # 1) token embedding\n",
        "        token_embeddings = self.token_embed(x)\n",
        "\n",
        "        # 2) positional embedding\n",
        "        positions = tf.range(0, seq_len, dtype=tf.int32)[tf.newaxis, :]  # (1, seq_len)\n",
        "        pos_embeddings = self.pos_embed(positions)                       # (1, seq_len, embed_dim)\n",
        "\n",
        "        x_embed = token_embeddings + pos_embeddings\n",
        "        x_embed = self.dropout(x_embed, training=training)\n",
        "\n",
        "        # TransformerBlock\n",
        "        for block in self.blocks:\n",
        "            x_embed = block(x_embed, training=training)\n",
        "\n",
        "        # feed foward\n",
        "        logits = self.fc_out(x_embed)  # (batch, seq_len, vocab_size)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "COFayRCJTgM8"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "vocab_size = sp.vocab_size()\n",
        "#build model\n",
        "model = SimpleGPT(\n",
        "    vocab_size=vocab_size,\n",
        "    max_seq_len=seq_len,\n",
        "    embed_dim=128,\n",
        "    num_heads=4,\n",
        "    ff_dim=256,\n",
        "    num_layers=2,\n",
        "    dropout_rate=0.1\n",
        ")\n",
        "\n",
        "# loss function\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "J9RouSmeWwO4",
        "outputId": "dab184fc-59f4-4e48-d0ad-974818001ff4"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"simple_gpt_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"simple_gpt_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_4                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_5                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)                 │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_4                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_5                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "EPOCHS = 50\n",
        "history = model.fit(dataset, epochs=EPOCHS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYqLNK_gWoNl",
        "outputId": "698dd3d2-efca-4285-a667-88db339c9f3f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 8.1263\n",
            "Epoch 2/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.0619\n",
            "Epoch 3/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.8476\n",
            "Epoch 4/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.3690\n",
            "Epoch 5/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.9267\n",
            "Epoch 6/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.6328\n",
            "Epoch 7/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.4197\n",
            "Epoch 8/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.1943\n",
            "Epoch 9/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.0204\n",
            "Epoch 10/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8369\n",
            "Epoch 11/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6593\n",
            "Epoch 12/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4877\n",
            "Epoch 13/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.2932\n",
            "Epoch 14/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1268\n",
            "Epoch 15/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9594\n",
            "Epoch 16/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7991\n",
            "Epoch 17/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6299\n",
            "Epoch 18/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4829\n",
            "Epoch 19/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3250\n",
            "Epoch 20/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1832\n",
            "Epoch 21/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0754\n",
            "Epoch 22/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9466\n",
            "Epoch 23/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8141\n",
            "Epoch 24/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6876\n",
            "Epoch 25/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5927\n",
            "Epoch 26/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4902\n",
            "Epoch 27/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3993\n",
            "Epoch 28/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3108\n",
            "Epoch 29/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2519\n",
            "Epoch 30/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1540\n",
            "Epoch 31/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0851\n",
            "Epoch 32/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0386\n",
            "Epoch 33/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9820\n",
            "Epoch 34/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9412\n",
            "Epoch 35/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8854\n",
            "Epoch 36/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8586\n",
            "Epoch 37/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8142\n",
            "Epoch 38/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7919\n",
            "Epoch 39/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7431\n",
            "Epoch 40/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7106\n",
            "Epoch 41/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6730\n",
            "Epoch 42/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6622\n",
            "Epoch 43/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6318\n",
            "Epoch 44/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6068\n",
            "Epoch 45/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5831\n",
            "Epoch 46/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5777\n",
            "Epoch 47/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5662\n",
            "Epoch 48/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5336\n",
            "Epoch 49/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5150\n",
            "Epoch 50/50\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_bpe(model, sp_model, start_string, max_new_tokens=100, temperature=0.8):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        model: The trained GPT model (TensorFlow).\n",
        "        sp_model: The SentencePieceProcessor (BPE model).\n",
        "        start_string: The initial text prompt for generation.\n",
        "        max_new_tokens: The number of tokens to generate.\n",
        "        temperature: Controls randomness in sampling (>1.0 => more random, <1.0 => more conservative).\n",
        "    Returns:\n",
        "        output_text: The final generated text string.\n",
        "    \"\"\"\n",
        "    # Convert the start string to token IDs (ensuring out_type=int)\n",
        "    input_ids = sp_model.encode(start_string, out_type=int)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Expand to a batch of size 1\n",
        "        x = tf.expand_dims(input_ids, 0)  # shape: (1, current_seq_len)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        logits = model(x)  # shape: (1, current_seq_len, vocab_size)\n",
        "        logits = logits[:, -1, :]  # Take the logits at the last time step\n",
        "        logits = logits / temperature  # Scale by temperature\n",
        "\n",
        "        # Convert logits to probabilities and sample\n",
        "        probs = tf.nn.softmax(logits, axis=-1)\n",
        "        next_token_id = tf.random.categorical(tf.math.log(probs), num_samples=1)[0, 0].numpy()\n",
        "\n",
        "        # Ensure it is an integer before appending\n",
        "        next_token_id = int(next_token_id)\n",
        "        input_ids.append(next_token_id)\n",
        "\n",
        "    # (Optional) Print for debugging:\n",
        "    # print(\"input_ids:\", input_ids)\n",
        "    # print(\"Element types:\", [type(e) for e in input_ids])\n",
        "\n",
        "    # Decode the final sequence of token IDs back into text\n",
        "    input_ids = [int(i) for i in input_ids]  # Ensure all are ints\n",
        "    output_text = sp_model.decode_ids(input_ids)\n",
        "    return output_text\n"
      ],
      "metadata": {
        "id": "k9mKSmpeXQeN"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"what is justice? \"\n",
        "gen_text = generate_text_bpe(model, sp, prompt, max_new_tokens=50, temperature=0.8)\n",
        "print(\"result：\\n\", gen_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypte86tSX8c2",
        "outputId": "6d53e752-2aaf-4feb-978b-993ac8af7f77"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result：\n",
            " what is justice? I am in your duty: There is to be in what you would very true, if you say, if you say, if we were discovered what nature, if we may be the horse or any other States which is to have failed to know what\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Use pre-train model to get better results**"
      ],
      "metadata": {
        "id": "L8qQjsHQ8fHF"
      }
    }
  ]
}