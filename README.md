# plato-GPT
I  trained a custom GPT (Generative Pre-trained Transformer) model from scratch using the text of Plato's "Republic" as the primary dataset.

so this one it is only the pretrain model. i dont do any post train.

what is next?

i plan to get some paper about plato in philosophy to do STF on it. 

maybe come up with a RL method to make this model undertand plato better, but how to design the reward is big problem. 

i read the deepseek R1 paper. they reward by how many think . this it is so cool a way to aviod RL game the reward model. 
